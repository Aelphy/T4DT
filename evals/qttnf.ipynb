{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd9c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import h5py\n",
    "import kaolin\n",
    "import opt_einsum\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import trimesh\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyvista as pv\n",
    "pv.set_jupyter_backend('pythreejs')\n",
    "from t4dt.utils import sdf2mesh\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from t4dt.utils import sdf2mesh\n",
    "\n",
    "\n",
    "def get_qtt_reshape_plan(dim_grid_log2, qtt_group_mode_size=3):\n",
    "    dim_grid = 2 ** dim_grid_log2\n",
    "    num_factors = dim_grid_log2 * qtt_group_mode_size\n",
    "\n",
    "    shape_src = [dim_grid] * qtt_group_mode_size\n",
    "    shape_dst = [2 ** qtt_group_mode_size] * dim_grid_log2\n",
    "    shape_factors = [2] * num_factors\n",
    "\n",
    "    factor_ids = torch.arange(num_factors)\n",
    "    permute_factors_src_to_dst = factor_ids.reshape(qtt_group_mode_size, dim_grid_log2).T.reshape(-1).tolist()\n",
    "    permute_factors_dst_to_src = factor_ids.reshape(dim_grid_log2, qtt_group_mode_size).T.reshape(-1).tolist()\n",
    "\n",
    "    return {\n",
    "        'shape_factors': shape_factors,\n",
    "        'shape_src': shape_src,\n",
    "        'shape_dst': shape_dst,\n",
    "        'permute_factors_src_to_dst': permute_factors_src_to_dst,\n",
    "        'permute_factors_dst_to_src': permute_factors_dst_to_src,\n",
    "    }\n",
    "\n",
    "\n",
    "def tensor_order_to_qtt(x, plan):\n",
    "    x = x.reshape(plan['shape_factors'])\n",
    "    x = x.permute(plan['permute_factors_src_to_dst'])\n",
    "    x = x.reshape(plan['shape_dst'])\n",
    "    return x\n",
    "\n",
    "\n",
    "def tensor_order_from_qtt(x, plan):\n",
    "    x = x.reshape(plan['shape_factors'])\n",
    "    x = x.permute(plan['permute_factors_dst_to_src'])\n",
    "    x = x.reshape(plan['shape_src'])\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_tt_ranks(shape, max_rank=None):\n",
    "    if type(shape) not in (tuple, list) or len(shape) == 0:\n",
    "        raise ValueError(f'Invalid shape: {shape}')\n",
    "    if len(shape) == 1:\n",
    "        return [1, 1]\n",
    "    ranks_left = [1] + torch.cumprod(torch.tensor(shape), dim=0).tolist()\n",
    "    ranks_right = list(reversed([1] + torch.cumprod(torch.tensor(list(reversed(shape))), dim=0).tolist()))\n",
    "    ranks_tt = [min(a, b) for a, b in zip(ranks_left, ranks_right)]\n",
    "    if max_rank is not None:\n",
    "        ranks_tt = [min(r, max_rank) for r in ranks_tt]\n",
    "    return ranks_tt\n",
    "\n",
    "\n",
    "def gen_letter():\n",
    "    next_letter_id = 0\n",
    "    while True:\n",
    "        yield opt_einsum.get_symbol(next_letter_id)\n",
    "        next_letter_id += 1\n",
    "\n",
    "\n",
    "def shapes(input):\n",
    "    return [c.shape for c in input]\n",
    "\n",
    "\n",
    "def is_tt_shapes(\n",
    "        input_shapes,\n",
    "        inputs_with_batch_dim=None,\n",
    "        batch_size=None,\n",
    "        allow_loose_rank_left=False,\n",
    "        allow_loose_rank_right=False,\n",
    "):\n",
    "    if type(input_shapes) not in (tuple, list) or \\\n",
    "            any(len(s) != 3 for s in input_shapes) or \\\n",
    "            any(input_shapes[i-1][-1] != input_shapes[i][0] for i in range(1, len(input_shapes))):\n",
    "        return False\n",
    "    if not (allow_loose_rank_left or input_shapes[0][0] == 1):\n",
    "        return False\n",
    "    if not (allow_loose_rank_right or input_shapes[-1][-1] == 1):\n",
    "        return False\n",
    "    if inputs_with_batch_dim is not None and not (\n",
    "            type(batch_size) is int and\n",
    "            batch_size > 0 and\n",
    "            type(inputs_with_batch_dim) in (tuple, list) and\n",
    "            len(inputs_with_batch_dim) == len(input_shapes) and\n",
    "            all([type(b) is bool for b in inputs_with_batch_dim])\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_list_of_tensors(input):\n",
    "    return type(input) in (list, tuple) and all(torch.is_tensor(c) for c in input)\n",
    "\n",
    "\n",
    "def is_tt(input):\n",
    "    return is_list_of_tensors(input) and is_tt_shapes(shapes(input))\n",
    "\n",
    "\n",
    "def perf_report(equation, *shapes, einsum_opt_method='dp'):\n",
    "    _, pathinfo = opt_einsum.contract_path(equation, *shapes, shapes=True, optimize=einsum_opt_method)\n",
    "    out = {\n",
    "        'flops': int(pathinfo.opt_cost),\n",
    "        'size_max_intermediate': int(pathinfo.largest_intermediate),\n",
    "        'size_all_intermediate': int(sum(pathinfo.size_list)),\n",
    "        'equation': equation,\n",
    "        'input_shapes': shapes,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def compile_tt_contraction_fn(\n",
    "        input_shapes,\n",
    "        inputs_with_batch_dim=None,\n",
    "        batch_size=None,\n",
    "        allow_loose_rank_left=False,\n",
    "        allow_loose_rank_right=False,\n",
    "        last_core_is_payload=False,\n",
    "        output_modes_squeeze=False,\n",
    "        output_last_rank_keep=False,\n",
    "        einsum_opt_method='dp',\n",
    "        report_flops=False\n",
    "):\n",
    "    if not is_tt_shapes(input_shapes, inputs_with_batch_dim, batch_size, allow_loose_rank_left, allow_loose_rank_right):\n",
    "        raise ValueError(f'Operand shapes do not form a tensor train: {input_shapes=} {inputs_with_batch_dim=} '\n",
    "                         f'{batch_size=} {allow_loose_rank_left=} {allow_loose_rank_right=}')\n",
    "\n",
    "    have_batch_dim = inputs_with_batch_dim is not None and any(inputs_with_batch_dim)\n",
    "    letter_batch = None\n",
    "    letter = gen_letter()\n",
    "    if have_batch_dim:\n",
    "        letter_batch = next(letter)\n",
    "\n",
    "    equation_left = ''\n",
    "    equation_right = letter_batch if have_batch_dim else ''\n",
    "\n",
    "    letter_core_last_rank_right = None\n",
    "    input_shapes_with_batch_dim = []\n",
    "\n",
    "    for i in range(len(input_shapes)):\n",
    "        if inputs_with_batch_dim is not None and inputs_with_batch_dim[i]:\n",
    "            input_shapes_with_batch_dim.append([batch_size] + list(input_shapes[i]))\n",
    "        else:\n",
    "            input_shapes_with_batch_dim.append(list(input_shapes[i]))\n",
    "\n",
    "        letter_rank_left = next(letter) if i == 0 else letter_core_last_rank_right\n",
    "        letters_modes = [next(letter) for _ in range(len(input_shapes[i]) - 2)]\n",
    "        letter_rank_right = next(letter)\n",
    "        letter_core_last_rank_right = letter_rank_right\n",
    "        if i > 0:\n",
    "            equation_left += ','\n",
    "        if inputs_with_batch_dim is not None and inputs_with_batch_dim[i]:\n",
    "            equation_left += letter_batch\n",
    "        equation_left += letter_rank_left\n",
    "        equation_left += ''.join(letters_modes)\n",
    "        equation_left += letter_rank_right\n",
    "        if i == 0 and input_shapes[i][0] > 1:\n",
    "            equation_right += letter_rank_left\n",
    "        if output_modes_squeeze:\n",
    "            for c, si in zip(letters_modes, input_shapes[i][1:-1]):\n",
    "                if si > 1 or (last_core_is_payload and i == len(input_shapes) - 1):\n",
    "                    equation_right += c\n",
    "        else:\n",
    "            equation_right += ''.join(letters_modes)\n",
    "        if i == len(input_shapes) - 1 and (output_last_rank_keep or input_shapes[i][-1] > 1):\n",
    "            equation_right += letter_rank_right\n",
    "\n",
    "    equation = equation_left + '->' + equation_right\n",
    "    contraction_fn = opt_einsum.contract_expression(equation, *input_shapes_with_batch_dim, optimize=einsum_opt_method)\n",
    "\n",
    "    if report_flops:\n",
    "        report = perf_report(equation, *input_shapes_with_batch_dim, einsum_opt_method=einsum_opt_method)\n",
    "        return contraction_fn, report\n",
    "\n",
    "    return contraction_fn\n",
    "\n",
    "def convert_qtt_to_tensor(input, qtt_reshape_plan=None, fn_contract=None, checks=False):\n",
    "    if checks and not is_tt(input):\n",
    "        raise ValueError('Operand is not a tensor train')\n",
    "    if fn_contract is None:\n",
    "        input_shapes = shapes(input)\n",
    "        fn_contract = compile_tt_contraction_fn(input_shapes)\n",
    "    out = fn_contract(*input)\n",
    "    if qtt_reshape_plan is not None:\n",
    "        out = tensor_order_from_qtt(out, qtt_reshape_plan)\n",
    "    return out\n",
    "\n",
    "loaded = torch.load('/scratch2/data/qttnf_model.pt', map_location='cpu')\n",
    "\n",
    "\n",
    "def extract_frame_sdf_from_ttnf(\n",
    "        cores,\n",
    "        dim_grid,\n",
    "        frame_id,\n",
    "        checks=False,\n",
    "):\n",
    "    dim_grid_log2 = int(math.log2(dim_grid))\n",
    "    frame_id_bits = [1 if (frame_id & (1 << (dim_grid_log2 - 1 - n))) else 0 for n in range(dim_grid_log2)]\n",
    "    assert len(cores) == dim_grid_log2, shapes(cores)\n",
    "    cores = [c.reshape(c.shape[0], 2, 8, c.shape[-1]) for c in cores]\n",
    "    cores = [c[:, i, :, :].cpu() for c, i in zip(cores, frame_id_bits)]\n",
    "    plan = get_qtt_reshape_plan(dim_grid_log2, qtt_group_mode_size=3)\n",
    "    out = convert_qtt_to_tensor(cores, qtt_reshape_plan=plan, checks=checks)\n",
    "    return out\n",
    "\n",
    "rec = extract_frame_sdf_from_ttnf(list(loaded.values()), 512, 0)\n",
    "coords = torch.tensor([-0.9714, -0.6217, -1.0168,  0.9179,  1.2920,  0.6641])\n",
    "pl = pv.Plotter()\n",
    "pl.camera_position = [0, 5, 10]\n",
    "pl.camera.elevation = 0\n",
    "pl.camera.roll = 0\n",
    "pl.camera.azimuth = 0\n",
    "pl.camera.zoom(1.5)\n",
    "\n",
    "for j, i in enumerate([0, 142, 283]):\n",
    "    framei = extract_frame_sdf_from_ttnf(list(loaded.values()), 512, i)\n",
    "    tmeshi = sdf2mesh(framei, coords)\n",
    "    tmeshi.vertices += j * np.array([-1, 0, 1])\n",
    "    mesh = pv.wrap(tmeshi)\n",
    "\n",
    "    pl.add_mesh(mesh)\n",
    "pl.show(screenshot=f'perda.png', jupyter_backend='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a46abc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c56b62cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eb40513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b4dcb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "pv.set_jupyter_backend('pythreejs')\n",
    "\n",
    "rec = extract_frame_sdf_from_ttnf(list(loaded.values()), 512, 0)\n",
    "coords = torch.tensor([-0.9714, -0.6217, -1.0168,  0.9179,  1.2920,  0.6641])\n",
    "pl = pv.Plotter()\n",
    "pl.camera_position = [0, 5, 10]\n",
    "pl.camera.elevation = 0\n",
    "pl.camera.roll = 0\n",
    "pl.camera.azimuth = 0\n",
    "pl.camera.zoom(1.5)\n",
    "\n",
    "for j, i in enumerate([0, 142, 283]):\n",
    "    framei = extract_frame_sdf_from_ttnf(list(loaded.values()), 512, i)\n",
    "    tmeshi = sdf2mesh(framei, coords)\n",
    "    tmeshi.vertices += j * np.array([-1, 0, 1])\n",
    "    mesh = pv.wrap(tmeshi)\n",
    "\n",
    "    pl.add_mesh(mesh)\n",
    "pl.show(screenshot=f'perda.png', jupyter_backend='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad34186a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16])\n",
      "torch.Size([16, 16, 256])\n",
      "torch.Size([256, 16, 256])\n",
      "torch.Size([256, 16, 256])\n",
      "torch.Size([256, 16, 256])\n",
      "torch.Size([256, 16, 256])\n",
      "torch.Size([256, 16, 256])\n",
      "torch.Size([256, 16, 16])\n",
      "torch.Size([16, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "for core in list(loaded.values()):\n",
    "    print(core.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9ac0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
